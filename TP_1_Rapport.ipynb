{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b20aca7-43da-4e06-8a6f-f1ca5de6c104",
   "metadata": {},
   "source": [
    "# Rapport sur TP - Artus et Guilhem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd918d-a3d7-4e59-8e93-b703db09ddfb",
   "metadata": {},
   "source": [
    "## Partie 1 : copie du rapport intégré au code \n",
    "\n",
    "- Data train = taille images * nb images = 28 * 28 * nb images\n",
    "- Label train = taille finale du vecteur * nb images  = 10 * nb images\n",
    "- Logique similaire pour données de test\n",
    "- Taille entrée = taille de w = data_train nb de colonnes * label_train*nombre de colonnes = 784 * 10\n",
    "- Taille sortie = taille de b = 1 * label_train nb de colonnes = 1 * 10\n",
    "- Taille de x = taille images * 5\n",
    "- Taille de y = taille de 5 résultats = 5 * 10\n",
    "- Taille de t = comme y\n",
    "- Taille of grad = 5 * 10. Intuitivement celui-ci je ne l'ai pas, commment ce nb va-t-il augmenter quand nous allons appliquer la chain rule ?\n",
    "- \n",
    "\n",
    "### Ce que donne le code : \n",
    "Taille entrée = 784, taille sortie = 10\n",
    " size of data_train : torch.Size([63000, 784])\n",
    " size of label_train : torch.Size([63000, 10])\n",
    " size of data_test : torch.Size([7000, 784])\n",
    " size of label_test : torch.Size([7000, 10])\n",
    " size of w : torch.Size([784, 10])\n",
    " size of b : torch.Size([1, 10])\n",
    " size of x : torch.Size([5, 784])\n",
    " size of y : torch.Size([5, 10])\n",
    " size of grad : torch.Size([5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917181fa-f242-49e2-854f-0b0ec1c01d9b",
   "metadata": {},
   "source": [
    "## Partie 2 : Shallow Network - Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90335b3",
   "metadata": {},
   "source": [
    "Modèle. Linear(784,k) → ReLU → Linear(k,10) ; sortie linéaire (logits), perte de classification (CrossEntropyLoss).\n",
    "\n",
    "### Méthodologie.\n",
    "\n",
    "Split 80/10/10 (train/val/test).\n",
    "\n",
    "GridSearchCV (cv=2) sur \n",
    "lr ∈ {10e−2,10e−4}, \n",
    "nb_neuronnes_couche_caché ∈ {20,1000}\n",
    "batch ∈ {16,128}\n",
    "→ meilleur ~95.3% val\n",
    "\n",
    "Affinage manuel (budget calcul limité) autour des meilleurs candidats, avec suivi val et early-stopping implicite par observation de la courbe.\n",
    "\n",
    "### Hyperparamètres retenus.\n",
    "\n",
    "η=0.005\n",
    "nb_neuronnes_couche_caché = 500, batch = 64, 30 époques.\n",
    "Scores obtenus : test ≈ 98.13% (meilleur epoch), validation = 97.70%.\n",
    "\n",
    "### Influence observée.\n",
    "\n",
    "η\n",
    "η trop grand → oscillations; trop petit → convergence lente. \n",
    "η=0.005\n",
    "η=0.005 donne le meilleur compromis.\n",
    "\n",
    "nb_neuronnes_couche_caché : 20 sous-ajuste, 1000 commence à sur-ajuster; 500 maximise val.\n",
    "\n",
    "Batch 64 stabilise l’optimisation (16 plus bruité, 128 un peu moins performant).\n",
    "\n",
    "###  Remarques de bonne pratique.\n",
    "\n",
    "Utiliser CrossEntropyLoss avec labels entiers (plus adapté que MSE sur one-hot).\n",
    "\n",
    "Conserver un seed et répéter 2–3 runs si possible pour lisser l’aléa d’initialisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33525b77",
   "metadata": {},
   "source": [
    "## Partie 3 : Deep Learning\n",
    "\n",
    "Nous avons implémenté un réseau profond (au moins deux couches cachées) en utilisant PyTorch. L’architecture comporte trois couches cachées avec des tailles variables \n",
    "(a,b,c)\n",
    "(a,b,c) et la fonction d’activation ReLU. La sortie est normalisée avec un Softmax.\n",
    "\n",
    "Hyperparamètres fixés :\n",
    "\n",
    "Taux d’apprentissage \n",
    "η=0.001\n",
    "η=0.001\n",
    "\n",
    "Batch size = 64\n",
    "\n",
    "30 époques maximum avec arrêt précoce (si la précision test diminue)\n",
    "\n",
    "Optimiseur : SGD, fonction de perte MSE\n",
    "\n",
    "### Tests réalisés\n",
    "\n",
    "Nous avons testé plusieurs architectures : (500,250,50), (50,250,500) et (250,250,250).\n",
    "\n",
    "### Résultats :\n",
    "\n",
    "Meilleure précision obtenue ≈ 97.4 % sur le test, atteinte en moins de 10 époques.\n",
    "\n",
    "Les trois architectures donnent des résultats proches (entre 96.5 % et 97.4 %).\n",
    "\n",
    "L’arrêt précoce évite l’overfitting après ~8–10 époques.\n",
    "\n",
    "### Influence des hyperparamètres\n",
    "\n",
    "Profondeur / largeur des couches : plus de couches permet de dépasser rapidement 96 %, mais élargir les couches n’apporte qu’un faible gain.\n",
    "\n",
    "Taux d’apprentissage : fixé à 0.001, il assure une convergence rapide et stable.\n",
    "\n",
    "Batch size : 64 est un compromis raisonnable entre rapidité et stabilité.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Sur MNIST, un MLP profond atteint environ 97 % de précision en peu d’époques. La taille des couches a peu d’influence, alors que le choix des hyperparamètres d’entraînement (learning rate, batch size, arrêt précoce) joue un rôle plus important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ed2a9-2101-463f-8d32-d19deefb7457",
   "metadata": {},
   "source": [
    "## Partie 4 : CNN \n",
    "\n",
    "### Choix du modèle\n",
    "Conformément à l'ennoncé du TP, nous avons choisi d'implémenter un CNN en appliquant l'architecture LeNet. Notre travail a consisté à l'adpater à la taille de nos images, en prenant en compte les bonnes dimension. Pas besoin de padding dans notre cas car déjà inclu dans les données de base apparemment.\n",
    "\n",
    "#### Choix des couches et dimensions associées : \n",
    "- Convolution 1 (+relu) : entrée en dimension 1*28*28 (car niveaux de gris uniquement donc pas *3 comme en couleur), sortie en 24*24*6 après convolution (kernel_size = 5, stride =1)\n",
    "- Pooling 1 (+relu) : prends les données de Conv1 en entrée, sortie en 12*12*6 (kernel_size=2, stride=2)\n",
    "- Convolution 2 (+ relu) : entrée de taille précédente, sortie en 8*8*16 (kernel_size=5, stride=1)\n",
    "- Pooling 2 (+ relu) : entrée de taille précédente, sortie en 4*4*16 (kernel_size=2, stride=2)\n",
    "- Fully connected 1 (Linear layer) : input: 16 * 4 * 4, output: 120 \n",
    "- Fully connected 2 (Linear layer) : input: 120, output: 84 \n",
    "- Output layer (Fully connected) ; input: 84, output: 10\n",
    "\n",
    "Nous avons choisi une structure qui passe progressivement de 120 à 10 dimensions, car après test c'est ce qui semblait le mieux fonctionner.\n",
    "\n",
    "## Hyperparamètres\n",
    "\n",
    "Basé sur un grid-search, nous avons choisi : \n",
    "- Taille de batch : 64 (pour gagner en efficacité, batch size plus réduit n'apportait que peu à la qualité du modèle).\n",
    "- Nombre d'époques : 30 (même si stabilisation à partir de 15 epochs, même si solution optimale trouvée autour de 25 on pourrait justifier qu'au delà de 15 notre modèle commence à over-fiter)\n",
    "- Taux d’apprentissage : 0.005\n",
    "\n",
    "## Résultats d'entraînement\n",
    "\n",
    "L’évaluation de la précision sur l’ensemble test montre :  \n",
    "- Une progression rapide durant les premières epochs, avec une précision passant de ~36% à plus de 90% en 4 epochs.  \n",
    "- À partir de la 10e époque, la précision dépasse 95%.  \n",
    "- Après environ 15 epochs, la précision atteint un premier plateau aux alentours de 97-98%, première convergence du modèle.  \n",
    "- Au delà, les performances oscillent autour de la convergence avec semble-t-il une légère tendance à l'amélioration. Et ce sans sur-apprentissage marqué. Plus de puissance de calcul nous permettrait de vérifier si la performance continue vraiment à s'améliorer ou stagne au delà, mais pas besoin d'aller au delà de 15 epochs pour notre modèle.\n",
    "\n",
    "| Epoch | Précision test |\n",
    "|-------|----------------|\n",
    "| 1     | 36%            |\n",
    "| 4     | 91%            |\n",
    "| 10    | 95%            |\n",
    "| 15    | 97.6%          |\n",
    "| 25    | 98.2%          |\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Le CNN utilisé permet d’obtenir une précision satisfaisante (>97%) sur MNIST, qui est certes un jeu de données réputé simple. Donc une architecture simple, sans trop besoin d'optimisation d'hyper paramètres suffit amplement.\n",
    "\n",
    "L’entraînement est considéré comme terminé vers 15 epochs, où la précision se stabilise, ce qui permettrait d’économiser du temps de calcul sans perte significative de performance.  \n",
    "\n",
    "A remarquer cependant que notre CNN n'offre pas d'augmentation significative de performances, par rapport aux modèles explorés plus tôt. Il faudrait donc sur ces données privilégier des méthodes plus simples et moins coûteuses en en énergie. Ce n'est pas si surprenant car les données MNIST sont réputées comme \"simples\", utiliser un CNN complèxe semble donc overkill sur un tel dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
